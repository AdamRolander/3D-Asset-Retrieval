{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb1174",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, CLIPVisionModel, CLIPImageProcessor\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm  # Use tqdm notebook for nice progress bars\n",
    "\n",
    "# Setup Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a1ba0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "# Relative path from 'notebooks/' to 'data/'\n",
    "DATA_ROOT = \"../data/processed\"\n",
    "IMAGES_DIR = os.path.join(DATA_ROOT, \"images\")\n",
    "CSV_PATH = os.path.join(DATA_ROOT, \"final_metadata_10k.csv\")\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 3\n",
    "MAX_TEXT_LEN = 128\n",
    "PROJECTION_DIM = 384 # Must match MiniLM dimension\n",
    "VISION_MODEL_NAME = \"openai/clip-vit-base-patch16\"\n",
    "TEXT_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f15cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Cap3DDataset(Dataset):\n",
    "    def __init__(self, csv_path, images_dir, tokenizer_name, vision_model_name):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (str): Path to the metadata CSV.\n",
    "            images_dir (str): Directory containing the images.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.images_dir = images_dir\n",
    "        \n",
    "        # Initialize Processors\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.processor = CLIPImageProcessor.from_pretrained(vision_model_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        uid = row['uid'] # Assuming column name is 'uid' or index 0\n",
    "        text = row['text_description'] # Assuming column name is 'text_description' or index 1\n",
    "        \n",
    "        # Construct Image Path reliably\n",
    "        # We use the UID to find the file in the images directory\n",
    "        image_filename = f\"{uid}.png\"\n",
    "        image_path = os.path.join(self.images_dir, image_filename)\n",
    "        \n",
    "        # Load and Convert Image\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            # Fallback or error handling if an image is missing\n",
    "            print(f\"Warning: Image not found for UID {uid} at {image_path}\")\n",
    "            # Create a black dummy image to prevent crash (optional)\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "\n",
    "        # Process Image\n",
    "        # return_tensors=\"pt\" gives [1, 3, 224, 224], we squeeze to [3, 224, 224]\n",
    "        pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        \n",
    "        # Tokenize Text\n",
    "        text_inputs = self.tokenizer(\n",
    "            text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=MAX_TEXT_LEN, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': text_inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_inputs['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "# Sanity Check: Load dataset and print one sample\n",
    "try:\n",
    "    dataset = Cap3DDataset(CSV_PATH, IMAGES_DIR, TEXT_MODEL_NAME, VISION_MODEL_NAME)\n",
    "    print(f\"Dataset loaded successfully with {len(dataset)} samples.\")\n",
    "    sample = dataset[0]\n",
    "    print(\"Sample keys:\", sample.keys())\n",
    "    print(\"Image shape:\", sample['pixel_values'].shape)\n",
    "    print(\"Text shape:\", sample['input_ids'].shape)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please check your paths in Cell 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc3d8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MultiModalContrastiveModel(nn.Module):\n",
    "    def __init__(self, text_model_name, vision_model_name, projection_dim=384):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Text Encoder (MiniLM - 384D output)\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        \n",
    "        # 2. Vision Encoder (CLIP ViT - 768D output)\n",
    "        self.vision_encoder = CLIPVisionModel.from_pretrained(vision_model_name)\n",
    "        \n",
    "        # 3. Projection Head\n",
    "        # CLIP ViT-Base-Patch16 has hidden size 768\n",
    "        vision_hidden_size = self.vision_encoder.config.hidden_size\n",
    "        self.vision_projection = nn.Linear(vision_hidden_size, projection_dim)\n",
    "        \n",
    "        # Learnable Temperature (logit_scale)\n",
    "        # Initialized to log(1/0.07) approx 2.65, standard for contrastive learning\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * 2.6592)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        # --- Text Embedding ---\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Mean Pooling for MiniLM\n",
    "        token_embeddings = text_outputs.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        text_embeds = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "        # --- Image Embedding ---\n",
    "        vision_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        # Use pooled output (CLS token) from CLIP ViT\n",
    "        image_embeds_raw = vision_outputs.pooler_output \n",
    "        \n",
    "        # Project to 384D\n",
    "        image_embeds = self.vision_projection(image_embeds_raw)\n",
    "        \n",
    "        # --- Normalization ---\n",
    "        text_embeds = F.normalize(text_embeds, p=2, dim=1)\n",
    "        image_embeds = F.normalize(image_embeds, p=2, dim=1)\n",
    "        \n",
    "        return text_embeds, image_embeds, self.logit_scale.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd1da2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Data Loader ---\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0) # Set num_workers=0 for simple debugging first\n",
    "\n",
    "# --- 2. Model & Optimizer ---\n",
    "model = MultiModalContrastiveModel(TEXT_MODEL_NAME, VISION_MODEL_NAME, projection_dim=PROJECTION_DIM).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- 3. Training Loop ---\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        \n",
    "        # Forward Pass\n",
    "        text_embeds, image_embeds, logit_scale = model(input_ids, attention_mask, pixel_values)\n",
    "        \n",
    "        # Compute Similarity Matrix\n",
    "        # (batch_size, batch_size)\n",
    "        logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "        logits_per_image = logits_per_text.t()\n",
    "        \n",
    "        # Generate Labels\n",
    "        # The labels are just the diagonal indices [0, 1, 2, ... batch_size-1]\n",
    "        current_batch_size = input_ids.shape[0]\n",
    "        labels = torch.arange(current_batch_size).to(device)\n",
    "        \n",
    "        # Symmetric Contrastive Loss\n",
    "        loss_text = criterion(logits_per_text, labels)\n",
    "        loss_image = criterion(logits_per_image, labels)\n",
    "        total_loss = (loss_text + loss_image) / 2\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update Metrics\n",
    "        epoch_loss += total_loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": total_loss.item()})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68bf375",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a directory to save weights\n",
    "save_path = \"../models/contrastive_finetuned\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Save the state dict\n",
    "torch.save(model.state_dict(), os.path.join(save_path, \"model_state_dict.pth\"))\n",
    "\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
