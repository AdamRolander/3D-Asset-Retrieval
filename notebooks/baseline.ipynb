{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "build_index.py\n",
    "\n",
    "Builds a text-based retrieval index for the Cap3D captions.\n",
    "\n",
    "  1. Loads the Cap3D metadata CSV (final_metadata_10k.csv).\n",
    "  2. Loads the list of available UIDs (available_uids.txt) and filters rows.\n",
    "  3. Encodes all captions using a MiniLM text encoder.\n",
    "  4. L2-normalizes the embeddings and saves them to disk.\n",
    "  5. Builds a FAISS inner-product index over the normalized embeddings.\n",
    "  6. Saves metadata so index results can be mapped back to (uid, caption, image_path).\n",
    "  7. Runs a small demo query to sanity-check the index.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24422c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a48b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to text file containing one UID per line (matching asset zip names)\n",
    "AVAILABLE_UIDS_PATH = \"data/available_uids.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce220965",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Path to CSV with columns: uid, caption, image_path\n",
    "CAPTIONS_CSV_PATH = \"data/processed/final_metadata_10k.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f079ff5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6863cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_available_uids(path: str) -> set:\n",
    "    \"\"\"\n",
    "    Load the list of available UIDs from a text file.\n",
    "    path : str\n",
    "        Path to 'available_uids.txt', one UID per line.\n",
    "    Returns\n",
    "    set[str]\n",
    "        Set of UID strings. Empty set if the file does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] available_uids file not found at {path}.\")\n",
    "        print(\"       Proceeding without UID-based filtering.\")\n",
    "        return set()\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        uids = {line.strip() for line in f if line.strip()}\n",
    "\n",
    "    print(f\"[INFO] Loaded {len(uids)} available UIDs from {path}\")\n",
    "    return uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98695c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_and_filter_captions(captions_path: str, available_uids: set) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the captions CSV and optionally filter rows to a set of UIDs.\n",
    "\n",
    "    Parameters\n",
    "    captions_path : str\n",
    "        Path to the CSV file with columns: uid, caption, image_path.\n",
    "    available_uids : set[str]\n",
    "        Set of allowed UIDs. If empty, no filtering is applied.\n",
    "    Output: \n",
    "    pandas.DataFrame\n",
    "        DataFrame with at least ['uid', 'caption', 'image_path'] columns,\n",
    "        filtered to available_uids (if provided) and with missing captions removed.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(captions_path):\n",
    "        raise FileNotFoundError(f\"Captions CSV not found at {captions_path}\")\n",
    "\n",
    "    print(f\"[INFO] Loading captions from {captions_path} ...\")\n",
    "    df = pd.read_csv(captions_path)\n",
    "\n",
    "    print(f\"[INFO] Original rows in CSV: {len(df)}\")\n",
    "    print(f\"[INFO] Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    expected_cols = {\"uid\", \"caption\", \"image_path\"}\n",
    "    if not expected_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"Expected columns {expected_cols}, but got {df.columns.tolist()}\")\n",
    "\n",
    "    # Filter by available_uids if provided\n",
    "    if available_uids:\n",
    "        before = len(df)\n",
    "        df = df[df[\"uid\"].isin(available_uids)].reset_index(drop=True)\n",
    "        print(f\"[INFO] Rows after filtering by available_uids: {before} -> {len(df)}\")\n",
    "    else:\n",
    "        print(\"[INFO] No available_uids provided, skipping UID filtering.\")\n",
    "\n",
    "    # Drop rows with missing captions\n",
    "    df = df.dropna(subset=[\"caption\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n[INFO] Preview of filtered data:\")\n",
    "    print(df.head(5))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8a1a9",
   "metadata": {},
   "source": [
    "embedding and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b5b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(df: pd.DataFrame, batch_size: int = 64) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Encode all captions into MiniLM embeddings.\n",
    "    Input: \n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with at least a 'caption' column.\n",
    "    batch_size : int, optional\n",
    "        Number of captions to encode at once (controls speed vs. memory).\n",
    "\n",
    "    Output: \n",
    "    numpy.ndarray\n",
    "        Embedding matrix of shape (N, D),\n",
    "        where N = number of rows and D = embedding dimension.\n",
    "    \"\"\"\n",
    "    print(\"\\n[INFO] Loading MiniLM model (all-MiniLM-L6-v2)\")\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    captions = df[\"caption\"].tolist()\n",
    "    all_embeddings = []\n",
    "\n",
    "    print(f\"[INFO] Encoding {len(captions)} captions in batches of {batch_size}...\")\n",
    "    for i in tqdm(range(0, len(captions), batch_size)):\n",
    "        batch = captions[i : i + batch_size]\n",
    "        emb = model.encode(batch, show_progress_bar=False)\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "    print(f\"[INFO] Embedding matrix shape = {embeddings.shape}\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5604f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    L2-normalize each embedding vector so that cosine similarity\n",
    "    is equivalent to inner product.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : numpy.ndarray\n",
    "        Array of shape (N, D) with raw embeddings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Array of shape (N, D) where each row has L2 norm â‰ˆ 1.\n",
    "    \"\"\"\n",
    "    print(\"\\n[INFO] Normalizing embeddings (L2)...\")\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-12\n",
    "    embeddings_norm = embeddings / norms\n",
    "    return embeddings_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9f980",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "FAISS index building and metadata\n",
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_save_faiss_index(embeddings_norm: np.ndarray, output_path: str) -> faiss.IndexFlatIP:\n",
    "    \"\"\"\n",
    "    Build a FAISS inner-product index over L2-normalized embeddings and save it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings_norm : numpy.ndarray\n",
    "        Array of shape (N, D) with L2-normalized embeddings (float or float32).\n",
    "    output_path : str\n",
    "        File path where the FAISS index (.faiss) will be written.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    faiss.IndexFlatIP\n",
    "        The constructed FAISS index object.\n",
    "    \"\"\"\n",
    "    n, d = embeddings_norm.shape\n",
    "    print(f\"\\n[INFO] Building FAISS IndexFlatIP with {n} vectors of dimension {d}...\")\n",
    "\n",
    "    embeddings_f32 = embeddings_norm.astype(\"float32\")\n",
    "\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(embeddings_f32)\n",
    "\n",
    "    print(\"[INFO] Index built. Saving to:\", output_path)\n",
    "    faiss.write_index(index, output_path)\n",
    "    print(\"[INFO] FAISS index saved.\")\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c33f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata(df: pd.DataFrame, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save (uid, caption, image_path) metadata so FAISS results can be interpreted.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with columns ['uid', 'caption', 'image_path'].\n",
    "    output_path : str\n",
    "        Path to a .csv file where metadata will be stored.\n",
    "    \"\"\"\n",
    "    cols_to_save = [\"uid\", \"caption\", \"image_path\"]\n",
    "    meta_df = df[cols_to_save].copy()\n",
    "    meta_df.to_csv(output_path, index=False)\n",
    "    print(f\"[INFO] Saved metadata ({len(meta_df)} rows) to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1aecd",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "Small demo: load index and run a couple of text queries\n",
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0546001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_search_from_disk(top_k: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Load the FAISS index and metadata from disk and run a few example queries.\n",
    "\n",
    "    This function:\n",
    "      - encodes query text with the same MiniLM model,\n",
    "      - L2-normalizes the query embedding,\n",
    "      - runs FAISS inner-product search,\n",
    "      - prints the top-k (uid, caption) matches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    top_k : int, optional\n",
    "        Number of results to print per query.\n",
    "    \"\"\"\n",
    "    faiss_index_path = \"data/index/text_index.faiss\"\n",
    "    metadata_path = \"data/index/text_index_metadata.csv\"\n",
    "\n",
    "    if not os.path.exists(faiss_index_path):\n",
    "        print(f\"[ERROR] FAISS index not found at {faiss_index_path}\")\n",
    "        return\n",
    "    if not os.path.exists(metadata_path):\n",
    "        print(f\"[ERROR] Metadata file not found at {metadata_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n[INFO] Loading FAISS index from disk...\")\n",
    "    index = faiss.read_index(faiss_index_path)\n",
    "\n",
    "    print(\"[INFO] Loading metadata from disk...\")\n",
    "    meta_df = pd.read_csv(metadata_path)\n",
    "\n",
    "    print(\"[INFO] Loading MiniLM model for query encoding...\")\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    queries = [\n",
    "        \"white sofa with wooden legs\",\n",
    "        \"airplane\",\n",
    "        \"robot\",\n",
    "        \"dump truck\",\n",
    "    ]\n",
    "\n",
    "    for q in queries:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"[QUERY] {q}\")\n",
    "\n",
    "        q_emb = model.encode([q], show_progress_bar=False)\n",
    "        norms = np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-12\n",
    "        q_emb_norm = (q_emb / norms).astype(\"float32\")\n",
    "\n",
    "        scores, indices = index.search(q_emb_norm, top_k)\n",
    "        scores = scores[0]\n",
    "        indices = indices[0]\n",
    "\n",
    "        print(f\"[INFO] Top-{top_k} results:\")\n",
    "        for rank, (idx, score) in enumerate(zip(indices, scores), start=1):\n",
    "            row = meta_df.iloc[idx]\n",
    "            uid = row[\"uid\"]\n",
    "            caption = row[\"caption\"]\n",
    "            print(f\"  {rank}. uid={uid}  score={score:.4f}\")\n",
    "            print(f\"     caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a986297d",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "Main entry point\n",
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "      1. Load available UIDs.\n",
    "      2. Load and filter caption metadata.\n",
    "      3. Build text embeddings for all captions.\n",
    "      4. Normalize and save embeddings.\n",
    "      5. Build and save FAISS index.\n",
    "      6. Save metadata for later lookup.\n",
    "      7. Run a small demo query on the saved index.\n",
    "    \"\"\"\n",
    "    # 1. Load available UIDs\n",
    "    available_uids = load_available_uids(AVAILABLE_UIDS_PATH)\n",
    "\n",
    "    # 2. Load and filter captions\n",
    "    df = load_and_filter_captions(CAPTIONS_CSV_PATH, available_uids)\n",
    "    print(f\"\\n[SUMMARY] Final number of caption rows: {len(df)}\")\n",
    "\n",
    "    os.makedirs(\"data/index\", exist_ok=True)\n",
    "\n",
    "    # 3. Build embeddings\n",
    "    embeddings = build_embeddings(df)\n",
    "\n",
    "    # 4. Normalize and save embeddings\n",
    "    embeddings_norm = normalize_embeddings(embeddings)\n",
    "    embeddings_path = \"data/index/text_embeddings_norm.npy\"\n",
    "    np.save(embeddings_path, embeddings_norm)\n",
    "    print(f\"[INFO] Saved normalized embeddings to {embeddings_path}\")\n",
    "\n",
    "    # 5. Build and save FAISS index\n",
    "    faiss_index_path = \"data/index/text_index.faiss\"\n",
    "    build_and_save_faiss_index(embeddings_norm, faiss_index_path)\n",
    "\n",
    "    # 6. Save metadata\n",
    "    metadata_path = \"data/index/text_index_metadata.csv\"\n",
    "    save_metadata(df, metadata_path)\n",
    "\n",
    "    print(\"\\n[SUMMARY] Index + embeddings + metadata saved.\")\n",
    "\n",
    "    # 7. Run a quick demo search\n",
    "    demo_search_from_disk(top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
