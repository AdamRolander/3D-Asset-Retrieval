{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb4bf04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] File not found: data/index/text_index.faiss\n",
      "[WARNING] File not found: data/index/text_index_metadata.csv\n",
      "[WARNING] File not found: tests/baseline_robustness_tests.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set plot style for better visuals\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# --- Configuration ---\n",
    "FAISS_INDEX_PATH = \"data/index/text_index.faiss\"\n",
    "METADATA_PATH = \"data/index/text_index_metadata.csv\"\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "TESTS_JSON_PATH = \"tests/baseline_robustness_tests.json\"\n",
    "TOP_K_DEFAULT = 10\n",
    "\n",
    "# Helper to check paths\n",
    "def check_paths():\n",
    "    paths = [FAISS_INDEX_PATH, METADATA_PATH, TESTS_JSON_PATH]\n",
    "    for p in paths:\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"[WARNING] File not found: {p}\")\n",
    "        else:\n",
    "            print(f\"[OK] Found: {p}\")\n",
    "\n",
    "check_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28260020",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Index or Metadata file missing.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m index, meta_df, model\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Load resources globally for the notebook\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m index, meta_df, model = \u001b[43mload_resources\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mResources loaded successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mload_resources\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load FAISS index, metadata DataFrame, and MiniLM model.\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(FAISS_INDEX_PATH) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(METADATA_PATH):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIndex or Metadata file missing.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[INFO] Loading FAISS index...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m index = faiss.read_index(FAISS_INDEX_PATH)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Index or Metadata file missing."
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Resources\n",
    "def load_resources():\n",
    "    \"\"\"Load FAISS index, metadata DataFrame, and MiniLM model.\"\"\"\n",
    "    if not os.path.exists(FAISS_INDEX_PATH) or not os.path.exists(METADATA_PATH):\n",
    "        raise FileNotFoundError(\"Index or Metadata file missing.\")\n",
    "\n",
    "    print(\"[INFO] Loading FAISS index...\")\n",
    "    index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "\n",
    "    print(\"[INFO] Loading metadata...\")\n",
    "    meta_df = pd.read_csv(METADATA_PATH)\n",
    "    \n",
    "    print(f\"[INFO] Loading Model ({MODEL_NAME})...\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    return index, meta_df, model\n",
    "\n",
    "# Load resources globally for the notebook\n",
    "index, meta_df, model = load_resources()\n",
    "print(\"Resources loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11577769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Search and Encoding Functions\n",
    "def encode_and_normalize(model, query: str):\n",
    "    \"\"\"Encode query and L2-normalize.\"\"\"\n",
    "    q_emb = model.encode([query], show_progress_bar=False)\n",
    "    norms = np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-12\n",
    "    return (q_emb / norms).astype(\"float32\")\n",
    "\n",
    "def search(index, meta_df, model, query: str, top_k=10):\n",
    "    \"\"\"Query FAISS index and return formatted results.\"\"\"\n",
    "    q_emb = encode_and_normalize(model, query)\n",
    "    scores, indices = index.search(q_emb, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for rank, (idx, score) in enumerate(zip(indices[0], scores[0]), start=1):\n",
    "        if idx < len(meta_df):\n",
    "            row = meta_df.iloc[idx]\n",
    "            results.append({\n",
    "                \"rank\": rank,\n",
    "                \"uid\": row[\"uid\"],\n",
    "                \"caption\": row[\"caption\"],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "    return results\n",
    "\n",
    "def get_cosine_sim(model, q1, q2):\n",
    "    \"\"\"Compute cosine similarity between two strings.\"\"\"\n",
    "    emb1 = encode_and_normalize(model, q1)\n",
    "    emb2 = encode_and_normalize(model, q2)\n",
    "    return float(np.dot(emb1[0], emb2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35633964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Manual Sanity Check\n",
    "queries = [\"white sofa with wooden legs\", \"airplane\"]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\n[Query]: {q}\")\n",
    "    res = search(index, meta_df, model, q, top_k=3)\n",
    "    for r in res:\n",
    "        print(f\"  {r['rank']}. [{r['uid']}] {r['caption']} (Score: {r['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c224515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Robustness Testing Functions\n",
    "def run_robustness_test(tests_path, index, meta_df, model):\n",
    "    with open(tests_path, \"r\") as f:\n",
    "        tests = json.load(f)\n",
    "\n",
    "    results = []\n",
    "    sim_drops = []\n",
    "    \n",
    "    print(f\"Running {len(tests)} test scenarios...\")\n",
    "    \n",
    "    for test in tests:\n",
    "        orig_q = test[\"orig\"]\n",
    "        \n",
    "        # 1. Establish Ground Truth from Original Query (Top-1)\n",
    "        orig_res = search(index, meta_df, model, orig_q, top_k=1)\n",
    "        if not orig_res: continue\n",
    "        target_uid = orig_res[0][\"uid\"]\n",
    "        \n",
    "        # 2. Evaluate Variants\n",
    "        for v in test.get(\"variants\", []):\n",
    "            pert_q = v[\"query\"]\n",
    "            v_type = v.get(\"type\", \"unknown\")\n",
    "            \n",
    "            # Search\n",
    "            res = search(index, meta_df, model, pert_q, top_k=10)\n",
    "            \n",
    "            # Check if target UID is in results\n",
    "            rank = next((r[\"rank\"] for r in res if r[\"uid\"] == target_uid), None)\n",
    "            hit_at_10 = 1 if rank else 0\n",
    "            hit_at_1 = 1 if rank == 1 else 0\n",
    "            \n",
    "            # Cosine Drop\n",
    "            sim = get_cosine_sim(model, orig_q, pert_q)\n",
    "            drop = max(0, 1.0 - sim)\n",
    "            \n",
    "            results.append({\n",
    "                \"test_name\": test[\"name\"],\n",
    "                \"type\": v_type,\n",
    "                \"orig_query\": orig_q,\n",
    "                \"pert_query\": pert_q,\n",
    "                \"rank\": rank if rank else 11, # 11 indicates miss for top-10\n",
    "                \"hit_at_10\": hit_at_10,\n",
    "                \"hit_at_1\": hit_at_1,\n",
    "                \"sim_drop\": drop\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the test\n",
    "df_results = run_robustness_test(TESTS_JSON_PATH, index, meta_df, model)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualization of Robustness\n",
    "if not df_results.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot 1: Cosine Similarity Drop by Perturbation Type\n",
    "    sns.boxplot(data=df_results, x=\"type\", y=\"sim_drop\", ax=axes[0], palette=\"Blues\")\n",
    "    axes[0].set_title(\"Cosine Similarity Drop by Perturbation Type\")\n",
    "    axes[0].set_ylabel(\"Similarity Drop (Lower is Better)\")\n",
    "    axes[0].set_xlabel(\"Perturbation Type\")\n",
    "\n",
    "    # Plot 2: Retrieval Success Rate (R@10)\n",
    "    # Calculate success rate per type\n",
    "    success_rates = df_results.groupby(\"type\")[\"hit_at_10\"].mean().reset_index()\n",
    "    sns.barplot(data=success_rates, x=\"type\", y=\"hit_at_10\", ax=axes[1], palette=\"Greens\")\n",
    "    axes[1].set_title(\"Retrieval Success Rate (R@10) by Perturbation Type\")\n",
    "    axes[1].set_ylabel(\"Success Rate (Higher is Better)\")\n",
    "    axes[1].set_ylim(0, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Print Summary Metrics ---\n",
    "    print(\"\\n=== Robustness Summary ===\")\n",
    "    print(f\"Mean Similarity Drop: {df_results['sim_drop'].mean():.4f}\")\n",
    "    print(f\"Overall R@10 Consistency: {df_results['hit_at_10'].mean():.2%}\")\n",
    "    \n",
    "    # Calculate Robustness Ratio (RR)\n",
    "    # RR = Perturbed Accuracy / Original Accuracy (Assumed 100% since we derived GT from it)\n",
    "    print(f\"Robustness Ratio (Approx): {df_results['hit_at_10'].mean():.3f}\")\n",
    "else:\n",
    "    print(\"No results to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save Baseline Results\n",
    "df_results.to_csv(\"data/baseline_robustness_results.csv\", index=False)\n",
    "print(\"Baseline results saved for future comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
