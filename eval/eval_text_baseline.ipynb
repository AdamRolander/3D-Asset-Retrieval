{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a19bb870",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paabl\\OneDrive\\Documents\\GitHub\\3D-Asset-Retrieval\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13842f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "FAISS_INDEX_PATH = \"WHY AWWWWdata/index/text_index.faiss\"\n",
    "METADATA_PATH = \"data/index/text_index_metadata.csv\"\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "TESTS_JSON_PATH = \"tests/baseline_robustness_tests.json\"\n",
    "TOP_K_DEFAULT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9df64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resources():\n",
    "    \"\"\"\n",
    "    load FAISS index, metadata DataFrame, and MiniLM model.\n",
    "\n",
    "    returns:\n",
    "        index: FAISS index\n",
    "        meta_df: DataFrame with columns ['uid', 'caption', 'image_path']\n",
    "        model: SentenceTransformer model for query encoding\n",
    "    \"\"\"\n",
    "    if not os.path.exists(FAISS_INDEX_PATH):\n",
    "        raise FileNotFoundError(f\"FAISS index not found at {FAISS_INDEX_PATH}\")\n",
    "    if not os.path.exists(METADATA_PATH):\n",
    "        raise FileNotFoundError(f\"Metadata CSV not found at {METADATA_PATH}\")\n",
    "\n",
    "    print(\"[INFO] Loading FAISS index from disk...\")\n",
    "    index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "\n",
    "    print(\"[INFO] Loading metadata from disk...\")\n",
    "    meta_df = pd.read_csv(METADATA_PATH)\n",
    "    print(f\"[INFO] Loaded metadata with {len(meta_df)} rows.\")\n",
    "\n",
    "    print(f\"[INFO] Loading MiniLM model ({MODEL_NAME}) for query encoding...\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    return index, meta_df, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f85418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_normalize_query(model, query: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    encode a single query string with MiniLM and L2-normalize the embedding\n",
    "\n",
    "    input:\n",
    "        model: SentenceTransformer\n",
    "        query: text string\n",
    "\n",
    "    output:\n",
    "        q_emb_norm: NumPy array of shape (1, D), dtype float32\n",
    "    \"\"\"\n",
    "    q_emb = model.encode([query], show_progress_bar=False)\n",
    "    norms = np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-12\n",
    "    q_emb_norm = (q_emb / norms).astype(\"float32\")\n",
    "    return q_emb_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87aaabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(model, q1: str, q2: str) -> float:\n",
    "    \"\"\"\n",
    "    compute cosine similarity between two query strings\n",
    "    using the same MiniLM encoder and L2-normalization.\n",
    "    \"\"\"\n",
    "    emb1 = encode_and_normalize_query(model, q1)  # shape (1, D)\n",
    "    emb2 = encode_and_normalize_query(model, q2)  # shape (1, D)\n",
    "\n",
    "    # since they are normalized, cosine = dot product\n",
    "    sim = float(np.dot(emb1[0], emb2[0]))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4a6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity_drop(\n",
    "    model,\n",
    "    original_queries: list,\n",
    "    perturbed_queries: list\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity drop between original and perturbed queries.\n",
    "    \n",
    "    Args:\n",
    "        model: SentenceTransformer model\n",
    "        original_queries: List of original query strings\n",
    "        perturbed_queries: List of perturbed query strings (same length)\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            'similarities': list of floats,\n",
    "            'mean_similarity': float,\n",
    "            'mean_drop': float (1 - mean_similarity),\n",
    "            'std_similarity': float,\n",
    "            'min_similarity': float,\n",
    "            'max_similarity': float\n",
    "        }\n",
    "    \"\"\"\n",
    "    if len(original_queries) != len(perturbed_queries):\n",
    "        raise ValueError(\"Query lists must have same length\")\n",
    "    \n",
    "    similarities = []\n",
    "    for orig, pert in zip(original_queries, perturbed_queries):\n",
    "        sim = cosine_similarity(model, orig, pert)\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    \n",
    "    return {\n",
    "        'similarities': similarities.tolist(),\n",
    "        'mean_similarity': float(np.mean(similarities)),\n",
    "        'mean_drop': float(1.0 - np.mean(similarities)),\n",
    "        'std_similarity': float(np.std(similarities)),\n",
    "        'min_similarity': float(np.min(similarities)),\n",
    "        'max_similarity': float(np.max(similarities))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e35d936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(index, meta_df, model, query: str, top_k: int = TOP_K_DEFAULT):\n",
    "    \"\"\"\n",
    "    run a text query against the FAISS index and return top k results\n",
    "\n",
    "    inputs:\n",
    "        index: FAISS index (IndexFlatIP with normalized embeddings)\n",
    "        meta_df: DataFrame with metadata (uid, caption, image_path)\n",
    "        model: MiniLM SentenceTransformer\n",
    "        query: text query string\n",
    "        top_k: number of results to return\n",
    "\n",
    "    output:\n",
    "        results: list of dicts like:\n",
    "            {\n",
    "                \"rank\": int,\n",
    "                \"idx\": int,\n",
    "                \"uid\": str,\n",
    "                \"caption\": str,\n",
    "                \"score\": float,\n",
    "            }\n",
    "    \"\"\"\n",
    "    q_emb_norm = encode_and_normalize_query(model, query)\n",
    "    scores, indices = index.search(q_emb_norm, top_k)\n",
    "    scores = scores[0]\n",
    "    indices = indices[0]\n",
    "\n",
    "    results = []\n",
    "    for rank, (idx, score) in enumerate(zip(indices, scores), start=1):\n",
    "        row = meta_df.iloc[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"rank\": rank,\n",
    "                \"idx\": int(idx),\n",
    "                \"uid\": row[\"uid\"],\n",
    "                \"caption\": row[\"caption\"],\n",
    "                \"score\": float(score),\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97934240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_subset(\n",
    "    index,\n",
    "    meta_df,\n",
    "    model,\n",
    "    num_samples: int = 100,\n",
    "    top_k: int = 10,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    evaluate retrieval performance on a random subset of captions.\n",
    "\n",
    "    for each sampled row:\n",
    "        - use its caption as the query\n",
    "        - treat its UID as the \"correct\" asset\n",
    "        - run search(top_k)\n",
    "        - compute R@1, R@5, R@10 and Reciprocal Rank\n",
    "\n",
    "    returns:\n",
    "        dict with averaged metrics: R@1, R@5, R@10, MRR, num_samples\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(meta_df)\n",
    "    if num_samples > n:\n",
    "        num_samples = n\n",
    "\n",
    "    sampled_indices = rng.choice(n, size=num_samples, replace=False)\n",
    "\n",
    "    r_at_1 = 0\n",
    "    r_at_5 = 0\n",
    "    r_at_10 = 0\n",
    "    mrr_sum = 0.0\n",
    "    individual_scores = []\n",
    "\n",
    "    for count, idx in enumerate(sampled_indices, start=1):\n",
    "        row = meta_df.iloc[idx]\n",
    "        true_uid = row[\"uid\"]\n",
    "        query_caption = row[\"caption\"]\n",
    "\n",
    "        results = search(index, meta_df, model, query_caption, top_k=top_k)\n",
    "\n",
    "        # find rank of the correct UID in the results\n",
    "        rank_of_true = None\n",
    "        for r in results:\n",
    "            if r[\"uid\"] == true_uid:\n",
    "                rank_of_true = r[\"rank\"]\n",
    "                break\n",
    "\n",
    "        individual_scores.append({\n",
    "            'query': query_caption,\n",
    "            'true_uid': true_uid,\n",
    "            'rank': rank_of_true,\n",
    "            'found_in_top1': rank_of_true == 1 if rank_of_true else False,\n",
    "            'found_in_top5': rank_of_true <= 5 if rank_of_true else False,\n",
    "            'found_in_top10': rank_of_true <= 10 if rank_of_true else False,\n",
    "            'reciprocal_rank': 1.0 / rank_of_true if rank_of_true else 0.0\n",
    "        })\n",
    "\n",
    "        if rank_of_true is not None:\n",
    "            if rank_of_true <= 1:\n",
    "                r_at_1 += 1\n",
    "            if rank_of_true <= 5:\n",
    "                r_at_5 += 1\n",
    "            if rank_of_true <= 10:\n",
    "                r_at_10 += 1\n",
    "\n",
    "            # Reciprocal Rank\n",
    "            mrr_sum += 1.0 / rank_of_true\n",
    "\n",
    "        if count % 10 == 0:\n",
    "            print(f\"[INFO] Processed {count}/{num_samples} samples...\")\n",
    "\n",
    "    num = float(num_samples)\n",
    "    metrics = {\n",
    "        \"R@1\": r_at_1 / num,\n",
    "        \"R@5\": r_at_5 / num,\n",
    "        \"R@10\": r_at_10 / num,\n",
    "        \"MRR\": mrr_sum / num,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"individual_scores\": individual_scores,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a469c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_robustness_ratio(\n",
    "    index,\n",
    "    meta_df,\n",
    "    model,\n",
    "    original_queries: list,\n",
    "    perturbed_queries: list,\n",
    "    ground_truth_uids: list,\n",
    "    k_values: list = [1, 5, 10]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute Robustness Ratio: RR = R@k(perturbed) / R@k(original).\n",
    "    \n",
    "    Args:\n",
    "        index: FAISS index\n",
    "        meta_df: Metadata DataFrame\n",
    "        model: SentenceTransformer model\n",
    "        original_queries: List of original query strings\n",
    "        perturbed_queries: List of perturbed versions (same length)\n",
    "        ground_truth_uids: List of correct UIDs for each query\n",
    "        k_values: List of k values to test (default: [1, 5, 10])\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            'original': {'R@1': float, 'R@5': float, 'R@10': float},\n",
    "            'perturbed': {'R@1': float, 'R@5': float, 'R@10': float},\n",
    "            'robustness_ratios': {'RR@1': float, 'RR@5': float, 'RR@10': float},\n",
    "            'num_queries': int\n",
    "        }\n",
    "    \"\"\"\n",
    "    if len(original_queries) != len(perturbed_queries) != len(ground_truth_uids):\n",
    "        raise ValueError(\"All input lists must have same length\")\n",
    "    \n",
    "    def compute_recall_at_k(queries, uids, k):\n",
    "        \"\"\"Helper: compute R@k for a set of queries.\"\"\"\n",
    "        hits = 0\n",
    "        for query, true_uid in zip(queries, uids):\n",
    "            results = search(index, meta_df, model, query, top_k=k)\n",
    "            if any(r['uid'] == true_uid for r in results):\n",
    "                hits += 1\n",
    "        return hits / len(queries) if queries else 0.0\n",
    "    \n",
    "    # Compute R@k for original and perturbed queries\n",
    "    results = {\n",
    "        'original': {},\n",
    "        'perturbed': {},\n",
    "        'robustness_ratios': {},\n",
    "        'num_queries': len(original_queries)\n",
    "    }\n",
    "    \n",
    "    for k in k_values:\n",
    "        r_orig = compute_recall_at_k(original_queries, ground_truth_uids, k)\n",
    "        r_pert = compute_recall_at_k(perturbed_queries, ground_truth_uids, k)\n",
    "        \n",
    "        results['original'][f'R@{k}'] = r_orig\n",
    "        results['perturbed'][f'R@{k}'] = r_pert\n",
    "        results['robustness_ratios'][f'RR@{k}'] = r_pert / r_orig if r_orig > 0 else 0.0\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eccae79",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_robustness_tests(index, meta_df, model, tests_path, top_ks=(1, 5, 10)):\n",
    "    \"\"\"\n",
    "    Run robustness tests over a set of query families.\n",
    "\n",
    "    tests: list of dicts, each like:\n",
    "        {\n",
    "            \"name\": str,\n",
    "            \"orig\": str,\n",
    "            \"orig_type\": str (optional, default \"canonical\"),\n",
    "            \"variants\": [\n",
    "                {\"query\": str, \"type\": str},\n",
    "                ...\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    For each test:\n",
    "      1. Use original query to define target UID (top-1 result).\n",
    "      2. For original + each variant:\n",
    "         - Run search with top_k = max(top_ks).\n",
    "         - Find rank of target UID if present.\n",
    "         - Classify as R@1, R@5, R@10, or miss.\n",
    "\n",
    "    Returns:\n",
    "      results: list of dicts with keys:\n",
    "        - test_name\n",
    "        - query_type\n",
    "        - variant_label\n",
    "        - variant_type\n",
    "        - query\n",
    "        - rank\n",
    "        - success_level\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(tests_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tests = json.load(f)\n",
    "\n",
    "\n",
    "    max_k = max(top_ks)\n",
    "    top_ks_sorted = sorted(top_ks)\n",
    "\n",
    "    def classify_rank(rank):\n",
    "        if rank is None:\n",
    "            return \"miss\"\n",
    "        for k in top_ks_sorted:\n",
    "            if rank <= k:\n",
    "                return f\"R@{k}\"\n",
    "        return \"miss\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    def evaluate_query(query, query_type, variant_label, variant_type, target_uid, test_name):\n",
    "        res = search(index, meta_df, model, query, top_k=max_k)\n",
    "\n",
    "        rank = None\n",
    "        for r in res:\n",
    "            if r[\"uid\"] == target_uid:\n",
    "                rank = r[\"rank\"]\n",
    "                break\n",
    "\n",
    "        success = classify_rank(rank)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"test_name\": test_name,\n",
    "                \"query_type\": query_type,        # \"orig\" or \"variant\"\n",
    "                \"variant_label\": variant_label,  # \"orig\" or the variant query\n",
    "                \"variant_type\": variant_type,    # e.g. \"typo\", \"hypernym\", etc.\n",
    "                \"query\": query,\n",
    "                \"rank\": rank,\n",
    "                \"success_level\": success,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return rank, success\n",
    "\n",
    "    print(\"[INFO] Running robustness tests (R@1 / R@5 / R@10)...\")\n",
    "\n",
    "    # Collect data for aggregate metrics\n",
    "    all_original_queries = []\n",
    "    all_variant_queries = []\n",
    "    all_target_uids = []\n",
    "\n",
    "    for test in tests:\n",
    "        name = test.get(\"name\", \"UNKNOWN_TEST\")\n",
    "        orig_query = test[\"orig\"]\n",
    "        orig_type = test.get(\"orig_type\", \"canonical\")\n",
    "        variants = test.get(\"variants\", [])\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 80+\"\\n\")\n",
    "        print(f\"[TEST] {name}\")\n",
    "        print(f\"  Original query: {orig_query}\")\n",
    "\n",
    "        # determine target UID from original query (top-1 result)\n",
    "        orig_results = search(index, meta_df, model, orig_query, top_k=max_k)\n",
    "        orig_top = orig_results[0]\n",
    "        target_uid = orig_top[\"uid\"]\n",
    "        print(\n",
    "            \"  [ORIG TOP-1] uid={}  score={:.4f}\".format(\n",
    "                target_uid, orig_top[\"score\"]\n",
    "            )\n",
    "        )\n",
    "        print(\"               caption: {}\".format(orig_top[\"caption\"]))\n",
    "\n",
    "        # evaluate original query\n",
    "        orig_rank, orig_level = evaluate_query(\n",
    "            query=orig_query,\n",
    "            query_type=\"orig\",\n",
    "            variant_label=\"orig\",\n",
    "            variant_type=orig_type,\n",
    "            target_uid=target_uid,\n",
    "            test_name=name,\n",
    "        )\n",
    "        print(f\"\\n  => Original: success={orig_level}  rank={orig_rank}  type={orig_type}\")\n",
    "\n",
    "        # pretty print variants in a table-like format\n",
    "        if variants:\n",
    "            print(\"\\n  Variants:\")\n",
    "            # header\n",
    "            print(\"    {succ:<7}  {rank:<4}  {vtype:<20}  {query}\".format(\n",
    "                succ=\"success\",\n",
    "                rank=\"rank\",\n",
    "                vtype=\"type\",\n",
    "                query=\"query\",\n",
    "            ))\n",
    "            print(\"    {:-<7}  {:-<4}  {:-<20}  {:-<40}\".format(\"\", \"\", \"\", \"\"))\n",
    "\n",
    "            for v in variants:\n",
    "                q = v[\"query\"]\n",
    "                v_type = v.get(\"type\", \"unknown\")\n",
    "\n",
    "                rank_v, level_v = evaluate_query(\n",
    "                    query=q,\n",
    "                    query_type=\"variant\",\n",
    "                    variant_label=q,\n",
    "                    variant_type=v_type,\n",
    "                    target_uid=target_uid,\n",
    "                    test_name=name,\n",
    "                )\n",
    "\n",
    "                rank_str = \"-\" if rank_v is None else str(rank_v)\n",
    "                print(\n",
    "                    \"    {succ:<7}  {rank:<4}  {vtype:<20}  {query}\".format(\n",
    "                        succ=level_v,\n",
    "                        rank=rank_str,\n",
    "                        vtype=v_type[:20],\n",
    "                        query=q,\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Collect for aggregate metrics\n",
    "                all_original_queries.append(orig_query)\n",
    "                all_variant_queries.append(q)\n",
    "                all_target_uids.append(target_uid)\n",
    "\n",
    "    # Compute aggregate metrics after all tests\n",
    "    if all_variant_queries:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"AGGREGATE METRICS (across all tests):\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Compute δSim\n",
    "        delta_sim = compute_cosine_similarity_drop(\n",
    "            model,\n",
    "            all_original_queries,\n",
    "            all_variant_queries\n",
    "        )\n",
    "        \n",
    "        # Compute Robustness Ratio\n",
    "        rr_metrics = compute_robustness_ratio(\n",
    "            index, meta_df, model,\n",
    "            all_original_queries,\n",
    "            all_variant_queries,\n",
    "            all_target_uids,\n",
    "            k_values=[1, 5, 10]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nCosine Similarity Drop (δSim):\")\n",
    "        print(f\"  Mean similarity: {delta_sim['mean_similarity']:.4f}\")\n",
    "        print(f\"  Mean drop (δSim): {delta_sim['mean_drop']:.4f}\")\n",
    "        print(f\"  Std deviation:   {delta_sim['std_similarity']:.4f}\")\n",
    "        print(f\"  Range:           [{delta_sim['min_similarity']:.4f}, {delta_sim['max_similarity']:.4f}]\")\n",
    "        \n",
    "        print(f\"\\nRobustness Ratio (RR):\")\n",
    "        print(f\"  Original R@1:  {rr_metrics['original']['R@1']:.3f}\")\n",
    "        print(f\"  Perturbed R@1: {rr_metrics['perturbed']['R@1']:.3f}\")\n",
    "        print(f\"  RR@1:          {rr_metrics['robustness_ratios']['RR@1']:.3f}\")\n",
    "        print(f\"  RR@5:          {rr_metrics['robustness_ratios']['RR@5']:.3f}\")\n",
    "        print(f\"  RR@10:         {rr_metrics['robustness_ratios']['RR@10']:.3f}\")\n",
    "        print(f\"  Total queries: {rr_metrics['num_queries']}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6649f79f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # load index, metadata, and text encoder\n",
    "    index, meta_df, model = load_resources()\n",
    "\n",
    "    # quick sanity-check queries\n",
    "    test_queries = [\n",
    "        \"white sofa with wooden legs\",\n",
    "        \"airplane\",\n",
    "    ]\n",
    "\n",
    "    for q in test_queries:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"[QUERY] {q}\")\n",
    "        results = search(index, meta_df, model, q, top_k=5)\n",
    "        for r in results:\n",
    "            print(f\"  {r['rank']}. uid={r['uid']}  score={r['score']:.4f}\")\n",
    "            print(f\"     caption: {r['caption']}\")\n",
    "\n",
    "    # evaluation on a random subset\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"[INFO] Starting evaluation on random subset of captions...\")\n",
    "\n",
    "    metrics = evaluate_random_subset(\n",
    "        index,\n",
    "        meta_df,\n",
    "        model,\n",
    "        num_samples=100,\n",
    "        top_k=10,\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluation Summary ({} samples):\".format(metrics[\"num_samples\"]))\n",
    "    print(\"  R@1  = {:.3f}\".format(metrics[\"R@1\"]))\n",
    "    print(\"  R@5  = {:.3f}\".format(metrics[\"R@5\"]))\n",
    "    print(\"  R@10 = {:.3f}\".format(metrics[\"R@10\"]))\n",
    "    print(\"  MRR  = {:.3f}\".format(metrics[\"MRR\"]))\n",
    "\n",
    "    # small robustness check\n",
    "    run_robustness_tests(index, meta_df, model, TESTS_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "449f00dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "FAISS index not found at data/index/text_index.faiss",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# load index, metadata, and text encoder\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     index, meta_df, model = \u001b[43mload_resources\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# quick sanity-check queries\u001b[39;00m\n\u001b[32m      6\u001b[39m     test_queries = [\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwhite sofa with wooden legs\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mairplane\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mload_resources\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mload FAISS index, metadata DataFrame, and MiniLM model.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03m    model: SentenceTransformer model for query encoding\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(FAISS_INDEX_PATH):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFAISS index not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFAISS_INDEX_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(METADATA_PATH):\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMetadata CSV not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMETADATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: FAISS index not found at data/index/text_index.faiss"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
